# from langchain_openai import ChatOpenAI
# from langchain.embeddings.base import Embeddings
# from langchain_community.vectorstores import Milvus
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
# from langchain_core.pydantic_v1 import BaseModel, Field
# from dotenv import load_dotenv
# import json
# import time
# import openai
# import re
# from config import Config
#
# load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡
#
# def create_embedding(client, text):
#     """
#     Creates an embedding for the given text using the OpenAI client.
#     """
#     return client.embeddings.create(model="embedding", input=text).data[0].embedding
#
# def create_openai_client(api_key, base_url):
#     """
#     Creates and returns an OpenAI client.
#     """
#     return openai.Client(api_key=api_key, base_url=base_url)
#
# # 1. åˆ›å»ºMilvusçŸ¥è¯†åº“
# def create_milvus_knowledge_base():
#     """
#     åˆ›å»ºMilvuså‘é‡æ•°æ®åº“å­˜å‚¨æ—…æ¸¸çŸ¥è¯†
#     """
#     # Initialize clients
#     openai_client = create_openai_client(Config.OPENAI_API_KEY, Config.OPENAI_BASE_URL)
#
#     documents = [
#         {
#             "title": "æµ·å—å²›ç»å…¸4å¤©3æ™šè¡Œç¨‹",
#             "content": """**è¡Œç¨‹å®‰æ’ï¼š**
# Day1ï¼šæŠµè¾¾â†’æ¤°æµ·æ—¶å…‰ - å»ºè®®å…¥ä½ç™½æ²™æ´²åŒºåŸŸé…’åº—ï¼Œå‚æ™šæ¬£èµæ—¥è½
# Day2ï¼šæµ®æ½œä¸€æ—¥æ¸¸ - æ¨èçŠç‘šèŠ±å›­å’Œå°é±¼ç¾¤èšé›†ç‚¹ï¼ˆçº¦$50- $70/äººï¼Œå«åˆé¤ï¼‰
# Day3ï¼šæ–‡åŒ–æ‘+å¤œå¸‚ - å‚è§‚æµ·å—æ–‡åŒ–æ‘ï¼ˆ$10/ç¥¨ï¼‰ï¼Œæ™šä¸Šé€›å¤œå¸‚å“å°æµ·é²œ
# Day4ï¼šè´­ç‰©+è¿”ç¨‹ - ä¸Šåˆå¯å»è´­ç‰©è¡—ä¹°ä¼´æ‰‹ç¤¼
#
# **ä½å®¿æ¨èï¼š**
# ç»æµå‹é…’åº—Aï¼ˆ$50/æ™šï¼‰ï¼Œèˆ’é€‚å‹é…’åº—Bï¼ˆ$100-$150/æ™šï¼‰
# """
#         },
#         {
#             "title": "æµ·å—å²›æ—…æ¸¸é¢„ç®—æŒ‡å—",
#             "content": """**ä¸­ç­‰æ¶ˆè´¹æ°´å¹³4å¤©3æ™šäººå‡é¢„ç®—ï¼š**
# - ä½å®¿ï¼ˆèˆ’é€‚å‹ï¼‰ï¼š$300-$450ï¼ˆ$100-$150/æ™šÃ—3æ™šï¼‰
# - é¤é¥®ï¼š$130-$180ï¼ˆæ™®é€šé¤å…$10-$15/é¤ï¼Œæµ·é²œå¤§é¤$25-$40ï¼‰
# - æ´»åŠ¨ï¼š$70ï¼ˆæµ®æ½œ$50+æ–‡åŒ–æ‘$10ï¼‰
# - äº¤é€šï¼š$50-$70ï¼ˆå²›å†…Tutuè½¦/ç§Ÿæ‘©æ‰˜è½¦ï¼‰
# - æ‚è´¹ï¼š$50-$50ï¼ˆæ°´/å°åƒ/çºªå¿µå“ï¼‰
# **æ€»è®¡ï¼š$600-$800/äººï¼ˆä¸å«å›½é™…æœºç¥¨ï¼‰**
# æ—ºå­£ä»·æ ¼ä¸Šæµ®10%-20%
# """
#         },
#         {
#             "title": "æµ·å—å²›æ™¯ç‚¹æ´»åŠ¨ä»·æ ¼",
#             "content": """**çƒ­é—¨æ´»åŠ¨ä»·æ ¼æ¸…å•ï¼š**
# - æµ®æ½œä¸€æ—¥æ¸¸ï¼š$50-$70ï¼ˆå«è£…å¤‡ã€åˆé¤ã€æ¥é€ï¼‰
# - æ·±æ½œä½“éªŒï¼š$90-$120
# - æµ·å—æ–‡åŒ–æ‘é—¨ç¥¨ï¼š$10
# - æ£®æ—å…¬å›­é—¨ç¥¨ï¼š$15
# - äº¤é€šèˆ¹ç¥¨ä»·ï¼š$20/å•ç¨‹
# - ç§Ÿæ‘©æ‰˜è½¦ï¼š$15/å¤© + æ²¹è´¹
# """
#         },
#         {
#             "title": "æµ·å—å²›ä½å®¿é¤é¥®å‚è€ƒ",
#             "content": """**ä½å®¿ä»·æ ¼èŒƒå›´ï¼š**
# - ç»æµå‹ï¼š$40-$80/æ™šï¼ˆå¦‚é…’åº—Aï¼‰
# - èˆ’é€‚å‹ï¼š$100-$150/æ™šï¼ˆå¦‚é…’åº—Bï¼Œè¿‘æµ·æ»©ï¼‰
# - è±ªåå‹ï¼š$300+/æ™šï¼ˆå«æ—©é¤ï¼‰
#
# **é¤é¥®æ¶ˆè´¹æ°´å¹³ï¼š**
# - æ™®é€šé¤å…ï¼š$8-$15/é¤
# - æµ·é²œå¤§é¤ï¼š$25-$40/äºº
# - å¤œå¸‚å°åƒï¼š$3-$8/ä»½
# - ç“¶è£…æ°´ï¼š$1-$2
# """
#         },
#         {
#             "title": "æµ·å—å²›æ—…æ¸¸æ³¨æ„äº‹é¡¹",
#             "content": """**é‡è¦æç¤ºï¼š**
# 1. æœ€ä½³æ—…æ¸¸æ—¶é—´ï¼š11æœˆ-æ¬¡å¹´4æœˆï¼ˆæ—±å­£ï¼‰
# 2. å¿…å¤‡ç‰©å“ï¼šé˜²æ™’éœœã€æ³³è¡£ã€é˜²æ°´è¢‹ã€è½¬æ¢æ’å¤´
# 3. æ–‡åŒ–ä¹ ä¿—ï¼šè¿›å…¥æ‘è½éœ€è„±é‹ï¼Œå°Šé‡å½“åœ°ä¿¡ä»°
# 4. äº¤é€šï¼šå²›å†…ä¸»è¦é Tutuè½¦ï¼ˆèµ·æ­¥$5ï¼‰æˆ–ç§Ÿæ‘©æ‰˜è½¦
# 5. é€‚åˆäººç¾¤ï¼šæƒ…ä¾£/æœ‹å‹ï¼ˆæµ®æ½œæ´»åŠ¨ï¼‰ï¼Œäº²å­éœ€æ³¨æ„å®‰å…¨
# """
#         }
#     ]
#
#     from langchain_core.documents import Document
#     docs = [
#         Document(
#             page_content=f"{doc['title']}\n{doc['content']}",
#             metadata={"source": doc["title"], "category": "travel_guide"}
#         )
#         for doc in documents
#     ]
#
#     from langchain.text_splitter import RecursiveCharacterTextSplitter
#     # æ–‡æ¡£åˆ†å—
#     text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=500,
#         chunk_overlap=50,
#         separators=["\n\n", "\n", ". ", "!", "?"]
#     )
#     splits = text_splitter.split_documents(docs)
#
#     # è‡ªå®šä¹‰Embeddingsç±»
#     class CustomOpenAIEmbeddings(Embeddings):
#         def __init__(self, openai_client):
#             self.openai_client = openai_client
#
#         def embed_query(self, text):
#             return create_embedding(self.openai_client, text)
#
#         def embed_documents(self, texts):
#             return [self.embed_query(text) for text in texts]
#
#     # è¿æ¥å‚æ•° - æ ¹æ®ä½ çš„Milvusé…ç½®ä¿®æ”¹
#     MILVUS_HOST = "172.11.17.23"  # è¿æ¥ä½ çš„MilvusæœåŠ¡åœ°å€
#     MILVUS_PORT = "19530"
#     COLLECTION_NAME = "travel_knowledge_base"
#
#     # æ„å»ºMilvuså‘é‡æ•°æ®åº“
#     vector_db = Milvus.from_documents(
#         documents=splits,
#         embedding=CustomOpenAIEmbeddings(openai_client),
#         connection_args={"host": MILVUS_HOST, "port": MILVUS_PORT},
#         collection_name=COLLECTION_NAME,
#         drop_old=True  # å¦‚æœå­˜åœ¨åŒåé›†åˆåˆ™åˆ é™¤é‡å»º
#     )
#
#     # ç­‰å¾…ç´¢å¼•æ„å»ºå®Œæˆ
#     print("ç­‰å¾…Milvusç´¢å¼•æ„å»º...")
#     time.sleep(5)
#
#     return vector_db.as_retriever(search_kwargs={"k": 5})
#
# from langchain_core.messages import AIMessage
# # è¾“å‡ºæ¸…ç†å‡½æ•°
# def clean_llm_output(msg: AIMessage):
#     """
#     æ¸…é™¤JSONå¤–çš„æ‰€æœ‰å†…å®¹
#     """
#     text = msg.content
#     # æ–¹æ³•1ï¼šæå–é¦–ä¸ªå®Œæ•´JSONå¯¹è±¡
#     if '{' in text and '}' in text:
#         start = text.index('{')
#         end = text.index('}') + 1
#         return text[start:end]
#     # æ–¹æ³•2ï¼šæ­£åˆ™æå–
#     json_match = re.search(pattern=r'\{.*?\}', text)
#     return json_match.group(0) if json_match else text
#
# # 2. æ„å›¾è¯†åˆ«ç»„ä»¶ï¼ˆä¿æŒä¸å˜ï¼‰
# class IntentRecognitionOutput(BaseModel):
#     """
#     æ„å›¾è¯†åˆ«çš„è¾“å‡ºç»“æ„
#     """
#     primary_intents: list[str] = Field(description="ä¸»è¦æ„å›¾åˆ—è¡¨")
#     entities: dict = Field(description="è¯†åˆ«å‡ºçš„å®ä½“")
#     slots_to_fill: list[str] = Field(description="éœ€è¦å¡«å……çš„æ§½ä½")
#     implicit_needs: list[str] = Field(description="éšå«éœ€æ±‚")
#
# def create_intent_recognition_chain():
#     """
#     åˆ›å»ºæ„å›¾è¯†åˆ«é“¾
#     """
#     prompt = ChatPromptTemplate.from_messages([
#         ("system", """ä½ æ˜¯ä¸€ä¸ªæ—…æ¸¸é¢†åŸŸæ„å›¾è¯†åˆ«ä¸“å®¶ã€‚åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«ï¼š
# 1. ä¸»è¦æ„å›¾ï¼ˆPrimary Intentsï¼‰- ä»åˆ—è¡¨ä¸­é€‰æ‹©ï¼š[PlanItinerary(è¡Œç¨‹è§„åˆ’), EstimateBudget(é¢„ç®—ä¼°ç®—), FindAccommodation(æŸ¥æ‰¾ä½å®¿), FindActivities(æŸ¥æ‰¾æ´»åŠ¨), GetTravelTips(è·å–æ—…è¡Œå»ºè®®)]
# 2. å®ä½“ï¼ˆEntitiesï¼‰- å¦‚ç›®çš„åœ°ã€æ—¥æœŸç­‰
# 3. éœ€è¦å¡«å……çš„æ§½ä½ï¼ˆSlots to Fillï¼‰
# 4. éšå«éœ€æ±‚ï¼ˆImplicit Needsï¼‰
#
# è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼JSONæ ¼å¼ï¼Œä½¿ç”¨ä»¥ä¸‹ç»“æ„ï¼š
# {"primary_intents": ["intent1", "intent2"], "entities": {"key": "value"}, "slots_to_fill": ["slot1", "slot2"], "implicit_needs": ["need1", "need2"]}
#
# æ³¨æ„ï¼šåªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""),
#         ("user", "ç”¨æˆ·æŸ¥è¯¢ï¼š{query}")
#     ])
#
#     model = ChatOpenAI(temperature=0, model="coder")
#     parser = JsonOutputParser(pydantic_object=IntentRecognitionOutput)
#
#     return prompt | model | clean_llm_output | parser
#
# # 3. éœ€æ±‚æ”¹å†™ç»„ä»¶ï¼ˆä¿æŒä¸å˜ï¼‰
# def create_query_rewriting_chain():
#     """
#     åˆ›å»ºéœ€æ±‚æ”¹å†™é“¾
#     """
#     prompt = ChatPromptTemplate.from_messages([
#         ("system", """ä½ æ˜¯ä¸€ä¸ªæ—…æ¸¸åŠ©æ‰‹ï¼Œè´Ÿè´£å°†æ¨¡ç³Šçš„ç”¨æˆ·æŸ¥è¯¢æ”¹å†™ä¸ºé€‚åˆæ£€ç´¢çš„æŸ¥è¯¢ã€‚
# æ ¹æ®æ„å›¾è¯†åˆ«ç»“æœï¼Œç”Ÿæˆ2-3ä¸ªæ”¹å†™åçš„æŸ¥è¯¢ï¼Œè¦æ±‚ï¼š
# 1. å¡«è¡¥å¸¸è§æ§½ä½ï¼ˆå¦‚å¤©æ•°=4å¤©3æ™šï¼Œé¢„ç®—=ä¸­ç­‰ï¼‰
# 2. æ·»åŠ ç›¸å…³å…³é”®è¯ï¼ˆè¡Œç¨‹ã€é¢„ç®—ã€æ™¯ç‚¹ã€ä½å®¿ç­‰ï¼‰
# 3. è¦†ç›–ä¸åŒè§’åº¦ï¼ˆè¡Œç¨‹æ¨¡æ¿ vs é¢„ç®—æ˜ç»†ï¼‰
# 4. è¾“å‡ºæ ¼å¼ï¼š{"rewritten_queries": ["query1", "query2", "query3"]}
#
# åªè¾“å‡ºJSONï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""),
#         ("user", """åŸå§‹æŸ¥è¯¢ï¼š{query}
# æ„å›¾åˆ†æç»“æœï¼š{intent_analysis}""")
#     ])
#
#     model = ChatOpenAI(temperature=0.3, model="coder")
#     parser = JsonOutputParser()
#
#     return prompt | model | clean_llm_output | parser
#
# # 4. æœ€ç»ˆç”Ÿæˆç»„ä»¶ï¼ˆä¼˜åŒ–ï¼‰
# def create_generation_chain():
#     """
#     åˆ›å»ºæœ€ç»ˆå›ç­”ç”Ÿæˆé“¾
#     """
#     prompt = ChatPromptTemplate.from_messages([
#         ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æµ·å²›æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›ç­”ï¼š
# - ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{query}
# - æ„å›¾åˆ†æç»“æœï¼š{intent_analysis}
# - æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š{context}
#
# å›ç­”è¦æ±‚ï¼š
# 1. ç»“æ„åŒ–è¡Œç¨‹ï¼šæä¾›æ¸…æ™°çš„4å¤©3æ™šè¡Œç¨‹å®‰æ’ï¼ˆåŸºäºæ£€ç´¢ç»“æœï¼‰
# 2. è¯¦ç»†é¢„ç®—ï¼šåˆ†é¡¹ä¼°ç®—ä¸­ç­‰æ¶ˆè´¹æ°´å¹³çš„è´¹ç”¨ï¼ˆåŸºäºæ£€ç´¢ç»“æœï¼‰
# 3. å¤„ç†æ¨¡ç³Šæ€§ï¼šæ˜ç¡®è¯´æ˜æ–¹æ³•åŸºäº4å¤©3æ™šå’Œä¸­ç­‰é¢„ç®—
# 4. å¼•å¯¼ä¸ªæ€§åŒ–ï¼šè¯¢é—®å…´è¶£/é¢„ç®—/æ—¶é—´/åŒè¡Œäººå‘˜ç­‰ä¿¡æ¯
# 5. è¯­æ°”ï¼šä¸“ä¸šã€çƒ­æƒ…ã€æ˜“æ‡‚
# 6. æ ¼å¼ï¼šä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡å†…å®¹
#
# é‡è¦è§„åˆ™ï¼š
# - åªä½¿ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ æ•°æ®ï¼
# - å¦‚æœæ£€ç´¢ä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
# - åœ¨é¢„ç®—éƒ¨åˆ†ä½¿ç”¨è¡¨æ ¼æ¸…æ™°å±•ç¤º"""),
#     ])
#
#     model = ChatOpenAI(temperature=0.2, model="coder")
#     return prompt | model | StrOutputParser()
#
# # 5. å®Œæ•´RAGæµç¨‹ï¼ˆä¼˜åŒ–ï¼‰
# def travel_planning_rag(query: str):
#     """
#     å®Œæ•´çš„æ—…æ¸¸è§„åˆ’RAGæµç¨‹
#     """
#     # æ­¥éª¤1ï¼šåˆ›å»ºMilvusçŸ¥è¯†åº“æ£€ç´¢å™¨
#     retriever = create_milvus_knowledge_base()
#
#     # æ­¥éª¤2ï¼šæ„å›¾è¯†åˆ«
#     intent_chain = create_intent_recognition_chain()
#     intent_analysis = intent_chain.invoke({"query": query})
#
#     # æ­¥éª¤3ï¼šéœ€æ±‚æ”¹å†™
#     rewrite_chain = create_query_rewriting_chain()
#     rewrite_result = rewrite_chain.invoke({
#         "query": query,
#         "intent_analysis": json.dumps(intent_analysis, ensure_ascii=False)
#     })
#     rewritten_queries = rewrite_result["rewritten_queries"]
#
#     # æ­¥éª¤4ï¼šæ£€ç´¢ï¼ˆä½¿ç”¨æ”¹å†™åçš„æŸ¥è¯¢ï¼‰
#     all_contexts = []
#     for q in rewritten_queries:
#         contexts = retriever.invoke(q)
#         all_contexts.extend([ctx.page_content for ctx in contexts])
#
#     # å»é‡å¹¶é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
#     unique_contexts = list(dict.fromkeys(all_contexts))[:5]
#     context_str = "\n\n---\n\n".join(unique_contexts)
#
#     # æ­¥éª¤5ï¼šç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
#     generation_chain = create_generation_chain()
#     final_answer = generation_chain.invoke({
#         "query": query,
#         "intent_analysis": json.dumps(intent_analysis, indent=2, ensure_ascii=False),
#         "context": context_str
#     })
#
#     # è¿”å›è¯¦ç»†æ—¥å¿—ç”¨äºæ¼”ç¤º
#     return {
#         "original_query": query,
#         "intent_analysis": intent_analysis,
#         "rewritten_queries": rewritten_queries,
#         "retrieved_contexts": unique_contexts,
#         "final_answer": final_answer
#     }
#
# # 6. ä¸»ç¨‹åºæ‰§è¡Œ
# if __name__ == "__main__":
#     # ç”¨æˆ·åŸå§‹æŸ¥è¯¢
#     user_query = "å»æµ·å—å²›ç©å‡ å¤©æ€ä¹ˆå®‰æ’æ¯”è¾ƒå¥½ï¼Ÿå¤§æ¦‚è¦èŠ±å¤šå°‘é’±ï¼Ÿ"
#
#     print("=" * 88)
#     print(f"ç”¨æˆ·åŸå§‹æŸ¥è¯¢ï¼š{user_query}")
#     print("=" * 88)
#
#     # æ‰§è¡Œå®Œæ•´RAGæµç¨‹
#     result = travel_planning_rag(user_query)
#
#     # æ‰“å°è¯¦ç»†è¿‡ç¨‹
#     print("\nğŸ” æ„å›¾è¯†åˆ«ç»“æœï¼š")
#     print(json.dumps(result["intent_analysis"], indent=2, ensure_ascii=False))
#
#     print("\nğŸ“ éœ€æ±‚æ”¹å†™ç»“æœï¼š")
#     for i, q in enumerate(result["rewritten_queries"], 1):
#         print(f"æ”¹å†™æŸ¥è¯¢ ({i}): {q}")
#
#     print("\nğŸ“š æ£€ç´¢åˆ°çš„å…³é”®ä¿¡æ¯ï¼š")
#     for i, ctx in enumerate(result["retrieved_contexts"], 1):
#         print(f"\nä¸Šä¸‹æ–‡ç‰‡æ®µ ({i}):")
#         print(ctx.strip())
#
#     print("\nğŸ’¬ æœ€ç»ˆç”Ÿæˆçš„å›ç­”ï¼š")
#     print("-" * 50)
#     print(result["final_answer"])
#     print("-" * 50)